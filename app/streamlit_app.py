import os
import io
import time
import json
import zipfile
import hashlib
import tempfile
from typing import Optional

import streamlit as st
from dotenv import load_dotenv

load_dotenv()


# åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨å¤„ç†ç»“æœ
TEMP_DIR = os.path.join(tempfile.gettempdir(), "pdf_processor_cache")
os.makedirs(TEMP_DIR, exist_ok=True)


def get_file_hash(file_bytes: bytes, params: dict) -> str:
	"""ç”ŸæˆåŸºäºæ–‡ä»¶å†…å®¹å’Œå‚æ•°çš„å“ˆå¸Œå€¼"""
	content = file_bytes + json.dumps(params, sort_keys=True).encode('utf-8')
	return hashlib.md5(content).hexdigest()


def save_result_to_file(file_hash: str, result: dict) -> str:
	"""å°†å¤„ç†ç»“æœä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶"""
	filepath = os.path.join(TEMP_DIR, f"{file_hash}.json")
	with open(filepath, 'w', encoding='utf-8') as f:
		# ä¸ä¿å­˜pdf_bytesåˆ°æ–‡ä»¶ï¼Œåªä¿å­˜å…¶ä»–ä¿¡æ¯
		result_copy = result.copy()
		result_copy.pop('pdf_bytes', None)
		json.dump(result_copy, f, ensure_ascii=False, indent=2)
	return filepath


def load_result_from_file(file_hash: str) -> Optional[dict]:
	"""ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½å¤„ç†ç»“æœ"""
	filepath = os.path.join(TEMP_DIR, f"{file_hash}.json")
	if os.path.exists(filepath):
		try:
			with open(filepath, 'r', encoding='utf-8') as f:
				return json.load(f)
		except:
			return None
	return None


@st.cache_data
def cached_process_pdf(src_bytes: bytes, params: dict) -> dict:
	"""ç¼“å­˜çš„PDFå¤„ç†å‡½æ•°"""
	from app.services import pdf_processor

	file_hash = get_file_hash(src_bytes, params)
	column_padding = params.get("column_padding", 10)

	# å°è¯•ä»ç¼“å­˜æ–‡ä»¶åŠ è½½
	cached_result = load_result_from_file(file_hash)
	if cached_result and cached_result.get("status") == "completed":
		# å¦‚æœæœ‰ç¼“å­˜ï¼Œéœ€è¦é‡æ–°ç”ŸæˆPDF bytesï¼ˆå› ä¸ºbytesä¸èƒ½åºåˆ—åŒ–åˆ°JSONï¼‰
		try:
			result_bytes = pdf_processor.compose_pdf(
				src_bytes,
				cached_result["explanations"],
				params["right_ratio"],
				params["font_size"],
				font_path=(params.get("cjk_font_path") or None),
				render_mode=params.get("render_mode", "markdown"),
				line_spacing=params["line_spacing"],
				column_padding=column_padding
			)
			cached_result["pdf_bytes"] = result_bytes
			return cached_result
		except Exception as e:
			# ä»ç¼“å­˜é‡æ–°åˆæˆPDFå¤±è´¥ï¼Œè¿”å›é”™è¯¯ç»“æœ
			return {
				"status": "failed",
				"pdf_bytes": None,
				"explanations": {},
				"failed_pages": [],
				"error": f"ä»ç¼“å­˜é‡æ–°åˆæˆPDFå¤±è´¥: {str(e)}"
			}

	# æ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜æ— æ•ˆï¼Œé‡æ–°å¤„ç†
	try:
		explanations, preview_images, failed_pages = pdf_processor.generate_explanations(
			src_bytes=src_bytes,
			api_key=params["api_key"],
			model_name=params["model_name"],
			user_prompt=params["user_prompt"],
			temperature=params["temperature"],
			max_tokens=params["max_tokens"],
			dpi=params["dpi"],
			concurrency=params["concurrency"],
			rpm_limit=params["rpm_limit"],
			tpm_budget=params["tpm_budget"],
			rpd_limit=params["rpd_limit"],
			use_context=params.get("use_context", False),
			context_prompt=params.get("context_prompt", None),
		)

		result_bytes = pdf_processor.compose_pdf(
			src_bytes,
			explanations,
			params["right_ratio"],
			params["font_size"],
			font_path=(params.get("cjk_font_path") or None),
			render_mode=params.get("render_mode", "markdown"),
			line_spacing=params["line_spacing"],
			column_padding=column_padding
		)

		result = {
			"status": "completed",
			"pdf_bytes": result_bytes,
			"explanations": explanations,
			"failed_pages": failed_pages
		}

		# ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
		save_result_to_file(file_hash, result)

		return result

	except Exception as e:
		result = {
			"status": "failed",
			"pdf_bytes": None,
			"explanations": {},
			"failed_pages": [],
			"error": str(e)
		}
		return result


def setup_page():
	st.set_page_config(page_title="PDF è®²è§£æµ Â· Gemini 2.5 Pro", layout="wide")
	st.title("PDF è®²è§£æµ Â· Gemini 2.5 Pro")
	st.caption("é€é¡µç”Ÿæˆè®²è§£ï¼Œå³ä¾§ç•™ç™½æ’ç‰ˆï¼Œä¿æŒåŸPDFå‘é‡å†…å®¹")


def sidebar_form():
	with st.sidebar:
		st.header("å‚æ•°é…ç½®")
		api_key = st.text_input("GEMINI_API_KEY", value=os.getenv('GEMINI_API_KEY'),type="password")
		model_name = st.text_input("æ¨¡å‹å", value="gemini-2.5-pro")
		temperature = st.slider("æ¸©åº¦", 0.0, 1.0, 0.4, 0.1)
		max_tokens = st.number_input("æœ€å¤§è¾“å‡º tokens", min_value=256, max_value=8192, value=4096, step=256)
		dpi = st.number_input("æ¸²æŸ“DPI(ä»…ä¾›LLM)", min_value=96, max_value=300, value=180, step=12)
		right_ratio = st.slider("å³ä¾§ç•™ç™½æ¯”ä¾‹", 0.2, 0.6, 0.48, 0.01)
		font_size = st.number_input("å³æ å­—ä½“å¤§å°", min_value=8, max_value=20, value=20, step=1)
		line_spacing = st.slider("è®²è§£æ–‡æœ¬è¡Œè·", 0.6, 2.0, 1.2, 0.1)
		column_padding = st.slider("æ å†…è¾¹è·(åƒç´ )", 2, 16, 10, 1, help="æ§åˆ¶æ¯ä¸€æ å·¦å³å†…è¾¹è·ï¼Œé˜²æ­¢æ–‡å­—è¢«åˆ‡è¾¹")
		concurrency = st.slider("å¹¶å‘é¡µæ•°", 1, 50, 50, 1)
		rpm_limit = st.number_input("RPM ä¸Šé™(è¯·æ±‚/åˆ†é’Ÿ)", min_value=10, max_value=5000, value=150, step=10)
		tpm_budget = st.number_input("TPM é¢„ç®—(ä»¤ç‰Œ/åˆ†é’Ÿ)", min_value=100000, max_value=20000000, value=2000000, step=100000)
		rpd_limit = st.number_input("RPD ä¸Šé™(è¯·æ±‚/å¤©)", min_value=100, max_value=100000, value=10000, step=100)
		user_prompt = st.text_area("è®²è§£é£æ ¼/è¦æ±‚(ç³»ç»Ÿæç¤º)", value="è¯·ç”¨ä¸­æ–‡è®²è§£æœ¬é¡µpdfï¼Œå…³é”®è¯ç»™å‡ºè‹±æ–‡ï¼Œè®²è§£è¯¦å°½ï¼Œè¯­è¨€ç®€æ´æ˜“æ‡‚ã€‚è®²è§£è®©äººä¸€çœ‹å°±æ‡‚ï¼Œä¾¿äºå¿«é€Ÿå­¦ä¹ ã€‚è¯·é¿å…ä¸å¿…è¦çš„æ¢è¡Œï¼Œä½¿é¡µé¢ä¿æŒç´§å‡‘ã€‚")
		cjk_font_path = st.text_input("CJK å­—ä½“æ–‡ä»¶è·¯å¾„(å¯é€‰)", value="assets/fonts/SIMHEI.TTF")
		render_mode = st.selectbox("å³æ æ¸²æŸ“æ–¹å¼", ["text", "markdown"], index=1)
		
		st.divider()
		st.subheader("ä¸Šä¸‹æ–‡å¢å¼º")
		use_context = st.checkbox("å¯ç”¨å‰åå„1é¡µä¸Šä¸‹æ–‡", value=False, help="å¯ç”¨åï¼ŒLLMå°†åŒæ—¶çœ‹åˆ°å‰ä¸€é¡µã€å½“å‰é¡µå’Œåä¸€é¡µçš„å†…å®¹ï¼Œæé«˜è®²è§£è¿è´¯æ€§ã€‚ä¼šå¢åŠ APIè°ƒç”¨æˆæœ¬ã€‚")
		context_prompt_text = st.text_area("ä¸Šä¸‹æ–‡æç¤ºè¯", value="ä½ å°†çœ‹åˆ°å‰ä¸€é¡µã€å½“å‰é¡µå’Œåä¸€é¡µçš„å†…å®¹ã€‚è¯·ç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆè¿è´¯çš„è®²è§£ã€‚å½“å‰é¡µæ˜¯é‡ç‚¹è®²è§£é¡µé¢ï¼Œä½ ä¸éœ€è¦è·Ÿæˆ‘è®²ä¸Šä¸€é¡µã€ä¸‹ä¸€é¡µè®²äº†ä»€ä¹ˆã€‚", disabled=not use_context, help="ç‹¬ç«‹çš„ä¸Šä¸‹æ–‡è¯´æ˜æç¤ºè¯ï¼Œç”¨äºæŒ‡å¯¼LLMå¦‚ä½•å¤„ç†å¤šé¡µå†…å®¹ã€‚")
		
		return {
			"api_key": api_key,
			"model_name": model_name,
			"temperature": float(temperature),
			"max_tokens": int(max_tokens),
			"dpi": int(dpi),
			"right_ratio": float(right_ratio),
			"font_size": int(font_size),
			"line_spacing": float(line_spacing),
			"column_padding": int(column_padding),
			"concurrency": int(concurrency),
			"rpm_limit": int(rpm_limit),
			"tpm_budget": int(tpm_budget),
			"rpd_limit": int(rpd_limit),
			"user_prompt": user_prompt.strip(),
			"cjk_font_path": cjk_font_path.strip(),
			"render_mode": render_mode,
			"use_context": bool(use_context),
			"context_prompt": context_prompt_text.strip() if use_context else None,
		}


def main():
	setup_page()
	params = sidebar_form()
	column_padding_value = params.get("column_padding", 10)

	# æ˜¾ç¤ºå½“å‰å¤„ç†çŠ¶æ€
	batch_results = st.session_state.get("batch_results", {})
	if batch_results:
		total_files = len(batch_results)
		completed_files = sum(1 for r in batch_results.values() if r["status"] == "completed")
		failed_files = sum(1 for r in batch_results.values() if r["status"] == "failed")
		processing_files = sum(1 for r in batch_results.values() if r["status"] == "processing")

		if processing_files > 0:
			st.info(f"ğŸ”„ æ­£åœ¨å¤„ç†ä¸­... å·²å®Œæˆ: {completed_files}/{total_files} ä¸ªæ–‡ä»¶")
		elif completed_files > 0:
			st.success(f"âœ… å¤„ç†å®Œæˆï¼æˆåŠŸ: {completed_files} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥: {failed_files} ä¸ªæ–‡ä»¶")
		elif failed_files > 0:
			st.error(f"âŒ å¤„ç†å¤±è´¥ï¼å¤±è´¥: {failed_files} ä¸ªæ–‡ä»¶")

	# æ‰¹é‡ä¸Šä¼ æ¨¡å¼
	uploaded_files = st.file_uploader("ä¸Šä¼  PDF æ–‡ä»¶ (æœ€å¤š20ä¸ª)", type=["pdf"], accept_multiple_files=True)
	if len(uploaded_files) > 20:
		st.error("æœ€å¤šåªèƒ½ä¸Šä¼ 20ä¸ªæ–‡ä»¶")
		uploaded_files = uploaded_files[:20]
		st.warning("å·²è‡ªåŠ¨æˆªå–å‰20ä¸ªæ–‡ä»¶")

	col_run, col_save = st.columns([2, 1])

	# ä¸‹è½½é€‰é¡¹
	with col_save:
		st.subheader("ä¸‹è½½é€‰é¡¹")
		download_mode = st.radio(
			"ä¸‹è½½æ–¹å¼",
			["åˆ†åˆ«ä¸‹è½½", "æ‰“åŒ…ä¸‹è½½"],
			help="åˆ†åˆ«ä¸‹è½½ï¼šä¸ºæ¯ä¸ªPDFç”Ÿæˆå•ç‹¬ä¸‹è½½æŒ‰é’®\næ‰“åŒ…ä¸‹è½½ï¼šå°†æ‰€æœ‰PDFæ‰“åŒ…æˆZIPæ–‡ä»¶"
		)
		if download_mode == "æ‰“åŒ…ä¸‹è½½":
			zip_filename = st.text_input("ZIPæ–‡ä»¶å", value="æ‰¹é‡è®²è§£PDF.zip")

	# åˆå§‹åŒ–session_state
	if "batch_results" not in st.session_state:
		st.session_state["batch_results"] = {}  # {filename: {"pdf_bytes": bytes, "explanations": dict, "status": str, "failed_pages": list}}
	if "batch_processing" not in st.session_state:
		st.session_state["batch_processing"] = False
	if "batch_zip_bytes" not in st.session_state:
		st.session_state["batch_zip_bytes"] = None
	if "batch_json_results" not in st.session_state:
		st.session_state["batch_json_results"] = {}
	if "batch_json_processing" not in st.session_state:
		st.session_state["batch_json_processing"] = False
	if "batch_json_zip_bytes" not in st.session_state:
		st.session_state["batch_json_zip_bytes"] = None

	with col_run:
		if st.button("æ‰¹é‡ç”Ÿæˆè®²è§£ä¸åˆæˆ", type="primary", use_container_width=True, disabled=st.session_state.get("batch_processing", False)):
			if not uploaded_files:
				st.error("è¯·å…ˆä¸Šä¼  PDF æ–‡ä»¶")
				st.stop()
			if not params["api_key"]:
				st.error("è¯·åœ¨ä¾§è¾¹æ å¡«å†™ GEMINI_API_KEY")
				st.stop()

			st.session_state["batch_processing"] = True
			st.session_state["batch_results"] = {}
			st.session_state["batch_zip_bytes"] = None

			total_files = len(uploaded_files)
			st.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼šé€é¡µæ¸²æŸ“â†’ç”Ÿæˆè®²è§£â†’åˆæˆæ–°PDFï¼ˆä¿æŒå‘é‡ï¼‰")

			# å»¶åå¯¼å…¥ä»¥åŠ å¿«é¦–å±
			from app.services import pdf_processor

			# æ•´ä½“è¿›åº¦
			overall_progress = st.progress(0)
			overall_status = st.empty()

			# é™åˆ¶åŒæ—¶å¤„ç†çš„PDFæ•°é‡ï¼Œé¿å…APIè¿‡è½½
			max_concurrent_pdfs = min(5, total_files)  # æœ€å¤šåŒæ—¶å¤„ç†5ä¸ªPDF

			for i, uploaded_file in enumerate(uploaded_files):
				filename = uploaded_file.name
				st.session_state["batch_results"][filename] = {"status": "processing", "pdf_bytes": None, "explanations": {}, "failed_pages": [], "json_bytes": None}

				# æ›´æ–°æ•´ä½“è¿›åº¦
				overall_progress.progress(int((i / total_files) * 100))
				overall_status.write(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ {i+1}/{total_files}: {filename}")

				try:
					# è¯»å–æºPDF bytes
					src_bytes = uploaded_file.read()

					# éªŒè¯PDFæ–‡ä»¶æœ‰æ•ˆæ€§
					is_valid, validation_error = pdf_processor.validate_pdf_file(src_bytes)
					if not is_valid:
						st.session_state["batch_results"][filename] = {
							"status": "failed",
							"pdf_bytes": None,
							"explanations": {},
							"failed_pages": [],
							"error": f"PDFæ–‡ä»¶éªŒè¯å¤±è´¥: {validation_error}"
						}
						st.error(f"âŒ {filename} PDFæ–‡ä»¶æ— æ•ˆ: {validation_error}")
						continue

					# æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜
					file_hash = get_file_hash(src_bytes, params)
					cached_result = load_result_from_file(file_hash)

					if cached_result and cached_result.get("status") == "completed":
						st.info(f"ğŸ“‹ {filename} ä½¿ç”¨ç¼“å­˜ç»“æœ")
						# ä»ç¼“å­˜åŠ è½½ï¼Œéœ€è¦é‡æ–°åˆæˆPDF
						try:
							result_bytes = pdf_processor.compose_pdf(
								src_bytes,
								cached_result["explanations"],
								params["right_ratio"],
								params["font_size"],
								font_path=(params.get("cjk_font_path") or None),
								render_mode=params.get("render_mode", "markdown"),
								line_spacing=params["line_spacing"],
								column_padding=column_padding_value
							)
							st.session_state["batch_results"][filename] = {
								"status": "completed",
								"pdf_bytes": result_bytes,
								"explanations": cached_result["explanations"],
								"failed_pages": cached_result["failed_pages"],
								"json_bytes": None
							}
						except Exception as e:
							# ç¼“å­˜é‡æ–°åˆæˆå¤±è´¥ï¼Œæ ‡è®°ä¸ºå¤±è´¥å¹¶å°è¯•é‡æ–°å¤„ç†
							st.warning(f"ç¼“å­˜é‡æ–°åˆæˆå¤±è´¥ï¼Œå°è¯•é‡æ–°å¤„ç†: {str(e)}")
							st.session_state["batch_results"][filename] = {"status": "processing", "pdf_bytes": None, "explanations": {}, "failed_pages": []}
							# ç»§ç»­åˆ°ä¸‹é¢çš„é‡æ–°å¤„ç†é€»è¾‘
							cached_result = None
					else:
						# éœ€è¦é‡æ–°å¤„ç†
						with st.spinner(f"å¤„ç† {filename} ä¸­..."):
							result = cached_process_pdf(src_bytes, params)
							st.session_state["batch_results"][filename] = result

					result = st.session_state["batch_results"][filename]
					if result["status"] == "completed":
						st.success(f"âœ… {filename} å¤„ç†å®Œæˆï¼")
					if result["failed_pages"]:
						st.warning(f"âš ï¸ {filename} ä¸­ {len(result['failed_pages'])} é¡µç”Ÿæˆè®²è§£å¤±è´¥")
					else:
						st.error(f"âŒ {filename} å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

				except Exception as e:
					st.session_state["batch_results"][filename] = {
						"status": "failed",
						"pdf_bytes": None,
						"explanations": {},
						"failed_pages": [],
						"error": str(e)
					}
					st.error(f"âŒ {filename} å¤„ç†å¤±è´¥: {str(e)}")

			# å®Œæˆå¤„ç†
			overall_progress.progress(100)
			overall_status.write("æ‰¹é‡å¤„ç†å®Œæˆï¼")

			# ç»Ÿè®¡ç»“æœ
			completed = sum(1 for r in st.session_state["batch_results"].values() if r["status"] == "completed")
			failed = sum(1 for r in st.session_state["batch_results"].values() if r["status"] == "failed")

			if completed > 0:
				st.success(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼æˆåŠŸ: {completed} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥: {failed} ä¸ªæ–‡ä»¶")
			else:
				st.error("âŒ æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥")

			# é¢„ç”Ÿæˆæ¯ä¸ªæ–‡ä»¶çš„ json_bytesï¼Œå¹¶æ„å»ºZIPç¼“å­˜
			for fname, res in st.session_state["batch_results"].items():
				if res.get("status") == "completed" and res.get("explanations"):
					try:
						res["json_bytes"] = json.dumps(res["explanations"], ensure_ascii=False, indent=2).encode("utf-8")
					except Exception:
						res["json_bytes"] = None
			# ä»…å½“å­˜åœ¨æˆåŠŸé¡¹æ—¶æ„å»ºZIP
			completed_any = any(r.get("status") == "completed" and r.get("pdf_bytes") for r in st.session_state["batch_results"].values())
			if completed_any:
				zip_buffer = io.BytesIO()
				with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
					for fname, res in st.session_state["batch_results"].items():
						if res.get("status") == "completed" and res.get("pdf_bytes"):
							base_name = os.path.splitext(fname)[0]
							new_filename = f"{base_name}è®²è§£ç‰ˆ.pdf"
							zip_file.writestr(new_filename, res["pdf_bytes"])
							if res.get("json_bytes"):
								json_filename = f"{base_name}.json"
								zip_file.writestr(json_filename, res["json_bytes"])
				zip_buffer.seek(0)
				st.session_state["batch_zip_bytes"] = zip_buffer.getvalue()
			else:
				st.session_state["batch_zip_bytes"] = None

			st.session_state["batch_processing"] = False

	with col_save:
		# æ˜¾ç¤ºæ‰¹é‡å¤„ç†ç»“æœ
		batch_results = st.session_state.get("batch_results", {})
		if batch_results:
			st.subheader("ğŸ“‹ å¤„ç†ç»“æœæ±‡æ€»")

			# ç»Ÿè®¡ä¿¡æ¯
			total_files = len(batch_results)
			completed_files = sum(1 for r in batch_results.values() if r["status"] == "completed")
			failed_files = sum(1 for r in batch_results.values() if r["status"] == "failed")

			col_stat1, col_stat2, col_stat3 = st.columns(3)
			with col_stat1:
				st.metric("æ€»æ–‡ä»¶æ•°", total_files)
			with col_stat2:
				st.metric("æˆåŠŸå¤„ç†", completed_files)
			with col_stat3:
				st.metric("å¤„ç†å¤±è´¥", failed_files)

			# è¯¦ç»†ç»“æœåˆ—è¡¨
			with st.expander("æŸ¥çœ‹è¯¦ç»†ç»“æœ", expanded=False):
				for filename, result in batch_results.items():
					if result["status"] == "completed":
						st.success(f"âœ… {filename} - å¤„ç†æˆåŠŸ")
						if result["failed_pages"]:
							st.warning(f"  âš ï¸ {len(result['failed_pages'])} é¡µç”Ÿæˆè®²è§£å¤±è´¥")
					else:
						st.error(f"âŒ {filename} - å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

			# é‡è¯•å¤±è´¥çš„æ–‡ä»¶
			failed_files_list = [f for f, r in batch_results.items() if r["status"] == "failed"]
			if failed_files_list and not st.session_state.get("batch_processing", False):
				st.subheader("ğŸ”„ é‡è¯•å¤±è´¥çš„æ–‡ä»¶")
				if st.button(f"é‡è¯• {len(failed_files_list)} ä¸ªå¤±è´¥çš„æ–‡ä»¶", use_container_width=True):
					st.info(f"å¼€å§‹é‡è¯• {len(failed_files_list)} ä¸ªå¤±è´¥çš„æ–‡ä»¶...")

					# æ‰¾åˆ°åŸå§‹ä¸Šä¼ çš„æ–‡ä»¶
					retry_files = []
					for failed_filename in failed_files_list:
						for uploaded_file in uploaded_files:
							if uploaded_file.name == failed_filename:
								retry_files.append(uploaded_file)
								break

					if retry_files:
						from app.services import pdf_processor

						retry_progress = st.progress(0)
						retry_status = st.empty()

						for i, uploaded_file in enumerate(retry_files):
							filename = uploaded_file.name
							retry_progress.progress(int((i / len(retry_files)) * 100))
							retry_status.write(f"é‡è¯•æ–‡ä»¶ {i+1}/{len(retry_files)}: {filename}")

							try:
								src_bytes = uploaded_file.read()

								file_progress = st.progress(0)
								file_status = st.empty()

								def on_file_progress(done: int, total: int):
									pct = int(done * 100 / max(1, total))
									file_progress.progress(pct)
									file_status.write(f"{filename}: æ­£åœ¨ç”Ÿæˆè®²è§£ {done}/{total}")

								def on_file_log(msg: str):
									file_status.write(f"{filename}: {msg}")

								with st.spinner(f"é‡è¯• {filename} ä¸­..."):
									explanations, preview_images, failed_pages = pdf_processor.generate_explanations(
										src_bytes=src_bytes,
										api_key=params["api_key"],
										model_name=params["model_name"],
										user_prompt=params["user_prompt"],
										temperature=params["temperature"],
										max_tokens=params["max_tokens"],
										dpi=params["dpi"],
										concurrency=params["concurrency"],
										rpm_limit=params["rpm_limit"],
										tpm_budget=params["tpm_budget"],
										rpd_limit=params["rpd_limit"],
										on_progress=on_file_progress,
										on_log=on_file_log,
										use_context=params.get("use_context", False),
										context_prompt=params.get("context_prompt", None),
									)

									result_bytes = pdf_processor.compose_pdf(
										src_bytes,
										explanations,
										params["right_ratio"],
										params["font_size"],
										font_path=(params.get("cjk_font_path") or None),
										render_mode=params.get("render_mode", "markdown"),
										line_spacing=params["line_spacing"],
										column_padding=column_padding_value
									)

								st.session_state["batch_results"][filename] = {
									"status": "completed",
									"pdf_bytes": result_bytes,
									"explanations": explanations,
									"failed_pages": failed_pages
								}

								st.success(f"âœ… {filename} é‡è¯•æˆåŠŸï¼")
								if failed_pages:
									st.warning(f"âš ï¸ {filename} ä¸­ä»æœ‰ {len(failed_pages)} é¡µç”Ÿæˆè®²è§£å¤±è´¥")

								file_progress.empty()
								file_status.empty()

							except Exception as e:
								st.error(f"âŒ {filename} é‡è¯•ä»ç„¶å¤±è´¥: {str(e)}")

						retry_progress.progress(100)
						retry_status.write("é‡è¯•å®Œæˆï¼")

						# æ›´æ–°ç»Ÿè®¡
						completed_after_retry = sum(1 for r in st.session_state["batch_results"].values() if r["status"] == "completed")
						failed_after_retry = sum(1 for r in st.session_state["batch_results"].values() if r["status"] == "failed")
						st.success(f"é‡è¯•åç»“æœï¼šæˆåŠŸ {completed_after_retry} ä¸ªï¼Œå¤±è´¥ {failed_after_retry} ä¸ª")

					else:
						st.error("æ— æ³•æ‰¾åˆ°éœ€è¦é‡è¯•çš„æ–‡ä»¶")

		# ä¸‹è½½åŠŸèƒ½
		if batch_results and any(r["status"] == "completed" for r in batch_results.values()):
			st.subheader("ğŸ“¥ ä¸‹è½½ç»“æœ")

			if download_mode == "æ‰“åŒ…ä¸‹è½½":
				zip_bytes = st.session_state.get("batch_zip_bytes")
				st.download_button(
					label="ğŸ“¦ ä¸‹è½½æ‰€æœ‰PDFå’Œè®²è§£JSON (ZIP)",
					data=zip_bytes,
					file_name=zip_filename,
					mime="application/zip",
					use_container_width=True,
					disabled=st.session_state.get("batch_processing", False) or not bool(zip_bytes),
					key="download_all_zip"
				)

			else:  # åˆ†åˆ«ä¸‹è½½
				st.write("**åˆ†åˆ«ä¸‹è½½æ¯ä¸ªæ–‡ä»¶ï¼š**")
				for filename, result in batch_results.items():
					if result["status"] == "completed" and result["pdf_bytes"]:
						base_name = os.path.splitext(filename)[0]
						pdf_filename = f"{base_name}è®²è§£ç‰ˆ.pdf"
						json_filename = f"{base_name}.json"

						col_dl1, col_dl2 = st.columns(2)
						with col_dl1:
							st.download_button(
								label=f"ğŸ“„ {pdf_filename}",
								data=result["pdf_bytes"],
								file_name=pdf_filename,
								mime="application/pdf",
								use_container_width=True,
								disabled=st.session_state.get("batch_processing", False),
								key=f"download_pdf_{filename}"
							)
						with col_dl2:
							if result["explanations"]:
								json_bytes = result.get("json_bytes")
								st.download_button(
									label=f"ğŸ“ {json_filename}",
									data=json_bytes,
									file_name=json_filename,
									mime="application/json",
									use_container_width=True,
									disabled=st.session_state.get("batch_processing", False) or not bool(json_bytes),
									key=f"download_json_{filename}"
								)

		# å¯¼å…¥è®²è§£JSONåŠŸèƒ½ï¼ˆå…¼å®¹æ‰¹é‡å’Œå•æ–‡ä»¶æ¨¡å¼ï¼‰
		st.subheader("ğŸ“¤ å¯¼å…¥åŠŸèƒ½")
		uploaded_expl = st.file_uploader("å¯¼å…¥è®²è§£JSON(å¯é€‰)", type=["json"], key="expl_json")
		if uploaded_expl and st.button("åŠ è½½è®²è§£JSONåˆ°ä¼šè¯", use_container_width=True):
			try:
				data = json.loads(uploaded_expl.read().decode("utf-8"))
				# é”®è½¬ä¸º int
				st.session_state["explanations"] = {int(k): str(v) for k, v in data.items()}
				st.success("å·²åŠ è½½è®²è§£JSONåˆ°ä¼šè¯ï¼Œå¯ç›´æ¥é‡æ–°åˆæˆã€‚")

				# å¦‚æœå½“å‰æœ‰ä¸Šä¼ çš„PDFæ–‡ä»¶ï¼Œæç¤ºå¯ä»¥ç›´æ¥åˆæˆ
				if uploaded_files:
					st.info("ğŸ’¡ æ£€æµ‹åˆ°å·²ä¸Šä¼ PDFæ–‡ä»¶ï¼Œæ‚¨å¯ä»¥ç‚¹å‡»ä¸‹æ–¹çš„'ä»…é‡æ–°åˆæˆ'æŒ‰é’®æ¥ä½¿ç”¨å¯¼å…¥çš„è®²è§£ç›´æ¥ç”ŸæˆPDFã€‚")

			except Exception as e:
				st.error(f"åŠ è½½å¤±è´¥ï¼š{e}")

		# ä»…é‡æ–°åˆæˆåŠŸèƒ½ï¼ˆä½¿ç”¨å¯¼å…¥çš„è®²è§£JSONï¼‰
		if st.session_state.get("explanations") and uploaded_files:
			st.subheader("ğŸ”„ é‡æ–°åˆæˆ")
			if st.button("ä»…é‡æ–°åˆæˆï¼ˆä½¿ç”¨å¯¼å…¥çš„è®²è§£ï¼‰", use_container_width=True):
				st.info("å¼€å§‹ä½¿ç”¨å¯¼å…¥çš„è®²è§£é‡æ–°åˆæˆPDF...")

				from app.services import pdf_processor

				# ä¸ºæ¯ä¸ªä¸Šä¼ çš„æ–‡ä»¶ç”ŸæˆPDF
				recompose_results = {}
				recompose_progress = st.progress(0)
				recompose_status = st.empty()

				for i, uploaded_file in enumerate(uploaded_files):
					filename = uploaded_file.name
					recompose_progress.progress(int((i / len(uploaded_files)) * 100))
					recompose_status.write(f"é‡æ–°åˆæˆ {i+1}/{len(uploaded_files)}: {filename}")

					try:
						src_bytes = uploaded_file.read()
						result_bytes = pdf_processor.compose_pdf(
							src_bytes,
							st.session_state["explanations"],
							params["right_ratio"],
							params["font_size"],
							font_path=(params.get("cjk_font_path") or None),
							render_mode=params.get("render_mode", "markdown"),
							line_spacing=params["line_spacing"],
							column_padding=column_padding_value
						)

						recompose_results[filename] = {
							"status": "completed",
							"pdf_bytes": result_bytes,
							"explanations": st.session_state["explanations"].copy(),
							"failed_pages": []
						}

						st.success(f"âœ… {filename} é‡æ–°åˆæˆå®Œæˆï¼")

					except Exception as e:
						recompose_results[filename] = {
							"status": "failed",
							"pdf_bytes": None,
							"explanations": {},
							"failed_pages": [],
							"error": str(e)
						}
						st.error(f"âŒ {filename} é‡æ–°åˆæˆå¤±è´¥: {str(e)}")

				# ä¿å­˜é‡æ–°åˆæˆçš„ç»“æœ
				st.session_state["batch_results"] = recompose_results

				recompose_progress.progress(100)
				recompose_status.write("é‡æ–°åˆæˆå®Œæˆï¼")

				completed_recompose = sum(1 for r in recompose_results.values() if r["status"] == "completed")
				failed_recompose = sum(1 for r in recompose_results.values() if r["status"] == "failed")
				st.success(f"é‡æ–°åˆæˆç»“æœï¼šæˆåŠŸ {completed_recompose} ä¸ªæ–‡ä»¶ï¼Œå¤±è´¥ {failed_recompose} ä¸ªæ–‡ä»¶")

		# æ‰¹é‡æ ¹æ®JSONé‡æ–°ç”ŸæˆPDFï¼ˆå•æ¡†ä¸Šä¼  + æ™ºèƒ½é…å¯¹ï¼‰
		st.subheader("ğŸ“š æ‰¹é‡æ ¹æ®JSONé‡æ–°ç”ŸæˆPDFï¼ˆå•æ¡†ä¸Šä¼ ï¼‰")

		# å•ä¸€ä¸Šä¼ æ¡†ï¼šåŒæ—¶æ¥æ”¶ PDF ä¸ JSON
		uploaded_mixed = st.file_uploader(
			"ä¸Šä¼  PDF ä¸ JSONï¼ˆå¯æ··åˆæ‹–æ‹½ï¼‰",
			type=["pdf", "json"],
			accept_multiple_files=True,
			key="mixed_pdf_json"
		)

		MAX_BYTES = 209_715_200  # 200MB
		pdf_files, json_files = [], []
		if uploaded_mixed:
			for f in uploaded_mixed:
				if f.size and f.size > MAX_BYTES:
					st.error(f"{f.name} è¶…è¿‡200MBé™åˆ¶")
					continue
				name = f.name.lower()
				if name.endswith(".pdf"):
					pdf_files.append(f)
				elif name.endswith(".json"):
					json_files.append(f)

		# ç‰¹ä¾‹ï¼šæ°å¥½ 1 PDF + 1 JSON -> ç›´æ¥æˆå¯¹ï¼Œæ— éœ€åç§°æ£€æŸ¥
		def _build_and_run_with_pairs(pairs):
			from app.services import pdf_processor
			st.info("å¼€å§‹æ‰¹é‡æ ¹æ®JSONé‡æ–°ç”ŸæˆPDF...")
			st.session_state["batch_json_processing"] = True
			st.session_state["batch_json_results"] = {}
			st.session_state["batch_json_zip_bytes"] = None
			# å°†ç¡®è®¤é…å¯¹è½¬ä¸ºç°æœ‰æ‰¹å¤„ç†å…¥å£çš„ä¸¤ä¸ªåˆ—è¡¨ï¼Œå¹¶è®© JSON åä¸ PDF åŒååŒ¹é…
			pdf_data, json_data = [], []
			for pdf_obj, json_obj in pairs:
				pdf_name = pdf_obj.name
				json_alias = os.path.splitext(pdf_name)[0] + ".json"
				pdf_data.append((pdf_name, pdf_obj.read()))
				json_data.append((json_alias, json_obj.read()))
			batch_results = pdf_processor.batch_recompose_from_json(
				pdf_data,
				json_data,
				params["right_ratio"],
				params["font_size"],
				font_path=(params.get("cjk_font_path") or None),
				render_mode=params.get("render_mode", "markdown"),
				line_spacing=params["line_spacing"],
				column_padding=column_padding_value
			)
			st.session_state["batch_json_results"] = batch_results
			# æ„å»ºZIPç¼“å­˜
			completed_count = sum(1 for r in batch_results.values() if r["status"] == "completed" and r.get("pdf_bytes"))
			if completed_count > 0:
				zip_buffer = io.BytesIO()
				with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
					for filename, result in batch_results.items():
						if result["status"] == "completed" and result.get("pdf_bytes"):
							base_name = os.path.splitext(filename)[0]
							new_filename = f"{base_name}è®²è§£ç‰ˆ.pdf"
							zip_file.writestr(new_filename, result["pdf_bytes"])
				zip_buffer.seek(0)
				st.session_state["batch_json_zip_bytes"] = zip_buffer.getvalue()
			else:
				st.session_state["batch_json_zip_bytes"] = None
			st.session_state["batch_json_processing"] = False

		if pdf_files and json_files and len(pdf_files) == 1 and len(json_files) == 1:
			col_single_ok, _ = st.columns([2,1])
			with col_single_ok:
				if st.button("ğŸš€ å¼€å§‹ï¼ˆ1 å¯¹ 1 ç›´æ¥é…å¯¹ï¼‰", type="primary", use_container_width=True):
					_build_and_run_with_pairs([(pdf_files[0], json_files[0])])

		# æ­£å¸¸ï¼šå¤šæ–‡ä»¶ -> æ™ºèƒ½åŒ¹é… + å¯ç¼–è¾‘é…å¯¹
		import pandas as _pd
		from difflib import SequenceMatcher
		from pathlib import Path as _Path

		def _normalize_basename(name: str) -> str:
			base = _Path(name).stem
			base = base.lower().strip()
			for ch in [" ", "-", ".", "_", "(", ")"]:
				base = base.replace(ch, "")
			return base

		def _best_match_map(pdf_names, json_names, threshold: float = 0.25):
			mapping = {}
			jn_norm = {j: _normalize_basename(j) for j in json_names}
			for p in pdf_names:
				pn = _normalize_basename(p)
				best = None
				best_score = 0.0
				for j, jn in jn_norm.items():
					s = SequenceMatcher(None, pn, jn).ratio()
					if s > best_score:
						best_score = s
						best = j
				mapping[p] = best if best_score >= threshold else None
			return mapping

		if pdf_files or json_files:
			pdf_names = [f.name for f in pdf_files]
			json_names = [f.name for f in json_files]

			st.caption(f"å·²é€‰æ‹© PDF: {len(pdf_names)}ï¼ŒJSON: {len(json_names)}")

			if pdf_names and json_names and not (len(pdf_names) == 1 and len(json_names) == 1):
				# ç”Ÿæˆå»ºè®®åŒ¹é…
				suggest = _best_match_map(pdf_names, json_names, 0.25)
				# æ„å»ºå¯ç¼–è¾‘è¡¨
				options = ["(æœªé€‰æ‹©)"] + json_names
				rows = [{"PDFæ–‡ä»¶": p, "JSONé€‰æ‹©": suggest.get(p) or "(æœªé€‰æ‹©)"} for p in pdf_names]
				df = _pd.DataFrame(rows)
				edited = st.data_editor(
					df,
					use_container_width=True,
					column_config={
						"JSONé€‰æ‹©": st.column_config.SelectboxColumn("JSONé€‰æ‹©", options=options, required=True)
					},
					hide_index=True,
					key="pair_editor"
				)

				# æ ¡éªŒï¼šç¦æ­¢é‡å¤æˆ–æœªé€‰æ‹©
				chosen = [v for v in edited["JSONé€‰æ‹©"].tolist() if v != "(æœªé€‰æ‹©)"]
				dup = len(chosen) != len(set(chosen))
				miss = any(v == "(æœªé€‰æ‹©)" for v in edited["JSONé€‰æ‹©"].tolist())
				if dup:
					st.error("å­˜åœ¨é‡å¤é€‰æ‹©çš„ JSONï¼Œè¯·è°ƒæ•´ä¸ºä¸€ä¸€å¯¹åº”ã€‚")
				if miss:
					st.warning("æœ‰ PDF æœªé€‰æ‹©å¯¹åº”çš„ JSONï¼Œå°†ä¸ä¼šè¢«å¤„ç†ã€‚")

				# è¿è¡ŒæŒ‰é’®
				if st.button("ğŸš€ å¼€å§‹æ‰¹é‡é‡æ–°ç”ŸæˆPDF", type="primary", use_container_width=True,
							disabled=st.session_state.get("batch_json_processing", False)):
					pairs = []
					json_map = {f.name: f for f in json_files}
					for _, row in edited.iterrows():
						pdf_name = row["PDFæ–‡ä»¶"]
						json_name = row["JSONé€‰æ‹©"]
						if json_name == "(æœªé€‰æ‹©)":
							continue
						pdf_obj = next((f for f in pdf_files if f.name == pdf_name), None)
						json_obj = json_map.get(json_name)
						if pdf_obj and json_obj:
							pairs.append((pdf_obj, json_obj))
					if not pairs:
						st.error("æ²¡æœ‰æœ‰æ•ˆçš„é…å¯¹å¯å¤„ç†ã€‚")
					else:
						_build_and_run_with_pairs(pairs)

		# æ˜¾ç¤ºæ‰¹é‡JSONå¤„ç†ç»“æœ
		batch_json_results = st.session_state.get("batch_json_results", {})
		if batch_json_results:
			st.subheader("ğŸ“¥ æ‰¹é‡JSONå¤„ç†ç»“æœä¸‹è½½")
			# ç»Ÿè®¡ä¿¡æ¯
			total_files = len(batch_json_results)
			completed_files = sum(1 for r in batch_json_results.values() if r["status"] == "completed")
			failed_files = sum(1 for r in batch_json_results.values() if r["status"] == "failed")
			col_stat1, col_stat2, col_stat3 = st.columns(3)
			with col_stat1:
				st.metric("æ€»æ–‡ä»¶æ•°", total_files)
			with col_stat2:
				st.metric("æˆåŠŸå¤„ç†", completed_files)
			with col_stat3:
				st.metric("å¤„ç†å¤±è´¥", failed_files)
			if completed_files > 0:
				zip_filename = f"æ‰¹é‡JSONé‡æ–°ç”ŸæˆPDF_{time.strftime('%Y%m%d_%H%M%S')}.zip"
				zip_bytes = st.session_state.get("batch_json_zip_bytes")
				st.download_button(
					label="ğŸ“¦ ä¸‹è½½æ‰€æœ‰æˆåŠŸå¤„ç†çš„PDF (ZIP)",
					data=zip_bytes,
					file_name=zip_filename,
					mime="application/zip",
					use_container_width=True,
					key="batch_json_zip_download",
					disabled=st.session_state.get("batch_json_processing", False) or not bool(zip_bytes)
				)
			st.write("**åˆ†åˆ«ä¸‹è½½æ¯ä¸ªæˆåŠŸå¤„ç†çš„æ–‡ä»¶ï¼š**")
			for filename, result in batch_json_results.items():
				if result["status"] == "completed" and result["pdf_bytes"]:
					base_name = os.path.splitext(filename)[0]
					pdf_filename = f"{base_name}è®²è§£ç‰ˆ.pdf"
					col_dl1, col_dl2 = st.columns([3, 1])
					with col_dl1:
						st.write(f"ğŸ“„ {pdf_filename}")
					with col_dl2:
						st.download_button(
							label="ä¸‹è½½",
							data=result["pdf_bytes"],
							file_name=pdf_filename,
							mime="application/pdf",
							key=f"batch_json_pdf_{filename}",
							disabled=st.session_state.get("batch_json_processing", False)
						)


if __name__ == "__main__":
	main()
